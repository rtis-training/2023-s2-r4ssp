---
title: "Transforming"
date: "Semester 1, 2023"
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 3
    code_download: true
    code_folding: show
---

```{r setup, include=FALSE}
library(knitr)

knitr::opts_chunk$set(
  comment = "#>",
  fig.path = "figures/06/", # use only for single Rmd files
  collapse = TRUE,
  echo = TRUE
)
```


> #### Associated Material
>
> Zoom notes: [Zoom Notes 06 - Transforming D](docs/zoom_notes_06_transform.html)
>
> Readings:
>
> - [R for Data Science Chapter 10](https://r4ds.had.co.nz/tibbles.html)
> - [R for Data Science Chapter 11](https://r4ds.had.co.nz/data-import.html)
> - [R for Data Science Chapter 12](https://r4ds.had.co.nz/tidy-data.html)
> - [R for Data Science Chapter 13](https://r4ds.had.co.nz/relational-data.html)
> - [R for Data Science Chapter 14](https://r4ds.had.co.nz/strings.html)

\


The pre-processing of data as a proportion of a data analysis workflow can be quite substantial, but this step is extremely vital - poor data in = poor results out. The main purpose of this module is transforming our data into something worthy of analysis and will cover data cleaning, tidying and 'reshaping'. This will be followed by how to increase the utility of our data by combining it with other datasets.

Let us first delve into the data cleaning and tidying.

\

## Cleaning/Tidying

The goal of cleaning and tidying our data is to deal with the imperfections that exist with real-world datasets and make them ready for analysis. Because we're focusing on how to do this inside R we're going to assume that you are already starting with 'rectangular' data - all rows are the same length, and all columns are the same length. Sometimes there is a 'pre-R' phase which requires you to make the data rectangular - check out this material from [Data Carpentry](https://datacarpentry.org/spreadsheet-ecology-lesson/) if this is something you first need to do.

If you have humans as part of the data collecting process it's only a matter of time before there is a data entry issue that needs to be 'cleaned' out.

Some common tasks at this stage include:

- ensuring that missing data is coded correctly
- dealing with typos
- removing unneeded whitespace
- converting columns to be the correct data type
- making nice column names

Unless you are in a fortunate position to inherit 'cleaned' data these jobs will fall on you to ensure that later on they don't cause issues when it comes time to do your statistics.

For this section we're going to make use of the following packages that are part of the Tidyverse: `readr` for reading in and parsing data, `tidyr` which has functions for dealing with missing data, and `stringr` which has functions relating to dealing with character vectors.

All are loaded as part of loading the `tidyverse` package. You can see under the "Attaching packages" heading exactly which packages are being loaded.

```{r}
library(tidyverse)
```

An extra package is `janitor`, which designed for cleaning.

```{r}
# install.packages("janitor")
library(janitor)
```


> The data for this section is derived from [untidy-portal-data.
xlsx](https://figshare.com/ndownloader/files/24469424). It is part of the [Portal Project Teaching Database](https://figshare.com/articles/Portal_Project_Teaching_Database/1314459) https://doi.org/10.6084/m9.figshare.1314459.v10 available on FigShare under a CC-0 license.
> 
> The original file does not have the data in a rectangular format so we have organised the data so it can be read into R and made it available to download either by navigating to [https://raw.githubusercontent.com/rtis-training/2023-s1-r4ssp/main/data/rodents_untidy.csv](https://raw.githubusercontent.com/rtis-training/2023-s1-r4ssp/main/data/rodents_untidy.csv) and using `Save As` "rodents_untidy.csv" in you data directory or by using the folowing R command (ensure you match the directory name to your directory set up):
> 
> ```{r, eval = FALSE}
> download.file(url = "https://raw.githubusercontent.com/rtis-training/2023-s1-r4ssp/main/data/rodents_untidy.csv", 
>       destfile = "data/rodents_untidy.csv")
> ```

Until now we've been using the `read.csv` ("read-dot-csv") function that is part of base R, but `readr` has equivalent functions, namely `read_csv` ("read-underscore-csv") which provides some extra features such as a progress bar, displaying the data types of all the columns as part of the reading, and it will create the special version of a data.frame called a `tibble`. For more in-depth details about `tibbles` check out the [R for Data Science - Tibbles](https://r4ds.had.co.nz/tibbles.html) chapter. The main benefit we'll utilise is the way it prints to the screen which includes some formatting and inclusion of the column data types under the column names.

```{r}
# note the use of the "read-underscore-csv"
rodents <- read_csv("data/rodents_untidy.csv")
```

The first thing we can look at as part of the data loading is the column names and what the datatype `read_csv` had a best guess at the type of data the column had.

```{r}
rodents %>% head()
```

It can be difficult if you have many columns to see them all, so instead you can see exactly what each column was loaded in as using `spec`.

```{r}
spec(rodents)
```

### Cleaning data values

From this list of columns, without knowing much about the data itself, _Weight_ might seen odd that it is a character type, rather than a numeric, so we'll focus in on this column to start with

```{r}
rodents$Weight
```

From this we can see that we have come "?" characters, is what has caused the column to be read in as characters instead of numbers (remember that a vector must be all the same data type).

Lets change the "?" to be a `NA`. We'll do this through sub-setting, where we find the elements that are "?" and assign them to now be `NA`.

```{r}
rodents$Weight[rodents$Weight == "?"] <- NA

rodents$Weight
```


Now that we have removed the characters, lets turn `rodents$Weight` into a numeric datatype

```{r}
rodents$Weight <- as.numeric(rodents$Weight)
```



Next as part of our cleaning we might want to change the `-999` entries in rodents$Weight to be `NA`

```{r}
rodents$Weight[rodents$Weight == -999] <- NA

rodents$Weight
```

\

### Cleaning names

You many have noticed that `read_csv` has kept the column names as they were in the file, and they actually violate the rules we have for variables in R - namely they contain a space - which can make dealing with them problematic.

```{r}
names(rodents)
```

We're now going to clean the names up, and this is where the `janitor` package is extremely helpful. Because we're only going to use a single function from the package we can call is directly using the `<package>::<function>` notation, rather than using `library`.


```{r}
rodents <- rodents %>% janitor::clean_names()

names(rodents)
```

> `janitor` has many options for how it cleans the names with the default being 'snake', but many others can be selected and supplied as the `case` parameter e.g. clean_names(case = 'lower_camel') for camel case starting with a lower case letter.

\

### Tidying

The three principles of tidy tabular data are:

1. Each column is a variable or property that is being measured 
2. Each row is an observation
3. A single cell should contain a single piece of information

Ideally these principles are followed from the very start when data collection is occurring

In our rodents dataset, the *plot_location* column violates rule number 3, it contains more than a single piece of information.

We can use the `separate` function from `tidyr` to split this column into 2 columns using the '_' as our field delimiter.

```{r}
rodents <- rodents %>% separate(plot_location, into = c("plot", "location"), sep = '_')

head(rodents)
```

Notice how the final 7 rows didn't have a location, and so were automatically filled with `NA`s. There is still more that could be done to clean and tidy this particular dataset but we'll leave it for now.

\

\


## Shaping Data

When we're thinking about the 'shape' of the data, we're thinking about in which direction are we adding observations - are we adding them as new columns onto the side making the data **wider**, or are we adding them as rows onto the bottom making the data **longer**?

In our framework for making tidy data we created a system that made **long** data, in that each row was an observation. Sometimes however we need to reshape this data into a **wide** format.


In the first example of this section we'll look at, we have yearly population data downloaded from Gapminder. 

> This data was originally downloaded from [Gapminder](https://www.gapminder.org/data/) and selecting the indicator "Population", then "Total population"
> 
> Either go to [this page](https://raw.githubusercontent.com/rtis-training/2023-s1-r4ssp/main/data/gapminder_yearly_population_millions_total.csv) and use `Save As` "gapminder_yearly_population_total.csv", saving it into your `data/` directory or by using the R command (adjust the directory "data" to match your directory name exactly):
> 
> ```{r, eval = FALSE}
> download.file(url = "https://raw.githubusercontent.com/rtis-training/2023-s1-r4ssp/main/data/gapminder_yearly_population_millions_total.csv", 
>       destfile = "data/gapminder_yearly_population_millions_total.csv")
> ```


```{r}
gapminder_yearly_pop <- read_csv("data/gapminder_yearly_population_millions_total.csv")
dim(gapminder_yearly_pop)
head(gapminder_yearly_pop)
```

### Wide to long

You can see that in this case we have a column for each year, and our rows relate to a particular country. This might be a valid for a final table, however it will cause some challenges if we want to perform some analysis or visualisation on it. While it is possible to perform operations across a row, it fights against how the dataframe structure itself is constructed - each column is a single vector and therefore has to be the same datatype, but a row is made of multiple vectors each of which could be a different datatype. Hence the majority of operations in R are column focused. 

E.g If we want to plot the change in population across time for a country is that in the current format we can't do it because we use a single column for each axis.

If we think to our tidy data principles, year is a variable that is being measured alongside population, so we should have a column 'year' into which we can store the year values.



The operation that will take us from a wide format to a long format is `pivot_longer` from `tidyr`. It has a `cols` parameter where we specify the columns that will be gathered into 2 named columns - one that will contain the names from the columns - `names_to`, the second, will contain the values that were in the columns - `values_to`. `cols` uses the same syntax as `select` from `dplyr` and if a `-` is used it will use all the columns _not_ specified.

```{r}
# Using the 'positive' selection of columns
gapminder_yearly_pop_long <- gapminder_yearly_pop %>% pivot_longer(cols = `1800`:`2100`, names_to = "year", values_to = "population")
gapminder_yearly_pop_long

# Using the 'negative' selection of columns
gapminder_yearly_pop_long <- gapminder_yearly_pop %>% pivot_longer(cols = -country, names_to = "year", values_to = "population")
gapminder_yearly_pop_long
```

Note that because the column names violate the rules for R variables in that they start with a number, we have to deal with the problematic names by enclosing them with backticks ``` ` ```.

### Long to wide

Sometimes we would like to take our data from a long format and into a wide format. For our example we want to create a table that will let us see distances between an origin and a destination within 500 miles of each other. To do this we're going to use the `flights` data from the `nycflights13` package.

The table that we create will essentially be a look-up table that will let us quickly reference the distance between origin/destination pairs. 


```{r}
# install.packages(nycflights13)
library(nycflights13)

glimpse(flights)
```



First lets select those three columns we need and filter so that the distance is within 500 miles.
```{r}
flights_long <- flights %>% select(origin, dest, distance) %>% 
  filter(distance <= 500)
```

Next we want remove duplicate rows, this can be done using `distinct()`
```{r}
flights_long <- flights_long %>% distinct()
```

You can see that our long data has `ncol(flights_long)` columns, and `nrow(flights_long)` rows.

```{r}
dim(flights_long)
```

We now want to make each of our destinations its own column and fill in the corresponding air time as the value. To do this, we can use `pivot_wider`. The main arguments are `names_from` which is the column you want take the new column names from, and `values_from` which is the column that will be used to fill in the new column values.

```{r}
flights_long %>% pivot_wider(origin, names_from = "dest", values_from = "distance")
```



